[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Learning rate and batch size are two crucial and integral parameters essential to finding the most optimized model. This blog aims to delve into the essential hyperparameters crucial for optimizing ML/neural network models specifically focusing on learning rate and batch size. However, before we compare these two let’s first revisit the distinction between model parameters and model hyperparameters, delve into learning rate and batch size individually, and then propose the most effective method for setting them in tandem.\n\n\nModel parameters comprise internal configuration variables within the model whose values are not manually specified; instead, they are learned automatically by the model. Examples include weights and biases.\n\n\n\nModel Hyperparameters consist of configuration variables whose values can be manually defined by the user. They are intended to assist the model in optimizing its Model Parameters effectively, encompassing elements such as learning rate and batch size.\n\n\n\nLearning rate is a scalar value that plays a pivotal role in dictating the size of steps taken in the direction of the negative gradient during the back-propagation process. The choice of learning rate can have a profound impact on the performance of a model. Opting for a high learning rate can result in rapid convergence, but it may also lead the model to overshoot the global minimum. Conversely, a low learning rate might require more time to converge and may even fail to reach the global minimum altogether. Therefore, selecting an appropriate learning rate becomes a critical task, one that seeks to strike a delicate balance between the effects of high and low learning rates.\n ### Batch size Batch size is a hyperparameter that governs the quantity of training samples processed before the model’s internal parameters receive updates. By setting the batch size to 1, the entire training dataset is treated as a single batch, and parameter updates occur after each individual sample. This approach is known as Stochastic Gradient Descent (SGD) and demands more memory and time. Conversely, setting the batch size to the total number of training samples results in each sample constituting a batch, with parameters being updated sequentially through each batch or sample. This approach is commonly known as Batch Gradient Descent. Between these extremes lies the concept of mini-batch processing, where the batch size, denoted as ‘n,’ falls between values greater than 1 and less than the total number of samples.\n\n\n\nHaving reviewed these key concepts, I believe we’ve laid the foundation for a better understanding. Now, let’s proceed to recommend the most effective approach.Discovering the optimal combination of learning rate and batch size typically involves a trial-and-error process. It demands a deep comprehension of the specific dataset, model architecture, and the problem domain. To embark on this journey, initiating with either a grid search or random search can systematically explore various combinations.Here are some suggestions to guide this exploration:\n1.Use Large Batch Size with Small Learning Rate :- A smaller learning rate means that the model updates its parameters more cautiously after each batch of data. This helps the model to converge more smoothly and avoid overshooting the optimal parameter values.\nUsing a large batch size (i.e., processing multiple data points simultaneously) can lead to a more stable gradient estimation. This stability can help in achieving better generalization because the model is exposed to a more representative sample of the training data during each update.\n2.Use Small Batch Size with Large Learning Rate: A large learning rate causes the model’s parameters to update more substantially after each batch. This can lead to faster convergence because the model explores the parameter space more aggressively.\nWith a small batch size, you’re updating the model’s parameters more frequently, which can help speed up convergence even further. Each batch contributes to an update, so the model can start improving quickly.\n\nKey takeaway\n\nSmall learning rate with large batch size ——–&gt; Good\nSmall learning rate with small batch size ——–&gt; Bad\nSmall Batch Size with large Learning Rate ——–&gt; Good\nSmall Batch Size with small Learning Rate ——–&gt; Bad"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html#model-parameters",
    "href": "posts/post-with-code/index.html#model-parameters",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Model parameters comprise internal configuration variables within the model whose values are not manually specified; instead, they are learned automatically by the model. Examples include weights and biases."
  },
  {
    "objectID": "posts/post-with-code/index.html#model-hyperparameters",
    "href": "posts/post-with-code/index.html#model-hyperparameters",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Model Hyperparameters consist of configuration variables whose values can be manually defined by the user. They are intended to assist the model in optimizing its Model Parameters effectively, encompassing elements such as learning rate and batch size."
  },
  {
    "objectID": "posts/post-with-code/index.html#learning-rate",
    "href": "posts/post-with-code/index.html#learning-rate",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Learning rate is a scalar value that plays a pivotal role in dictating the size of steps taken in the direction of the negative gradient during the back-propagation process. The choice of learning rate can have a profound impact on the performance of a model. Opting for a high learning rate can result in rapid convergence, but it may also lead the model to overshoot the global minimum. Conversely, a low learning rate might require more time to converge and may even fail to reach the global minimum altogether. Therefore, selecting an appropriate learning rate becomes a critical task, one that seeks to strike a delicate balance between the effects of high and low learning rates."
  },
  {
    "objectID": "posts/post-with-code/index.html#batch-size",
    "href": "posts/post-with-code/index.html#batch-size",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Batch size is a hyperparameter that governs the quantity of training samples processed before the model’s internal parameters receive updates. By setting the batch size to 1, the entire training dataset is treated as a single batch, and parameter updates occur after each individual sample. This approach is known as Stochastic Gradient Descent (SGD) and demands more memory and time. Conversely, setting the batch size to the total number of training samples results in each sample constituting a batch, with parameters being updated sequentially through each batch or sample. This approach is commonly known as Batch Gradient Descent. Between these extremes lies the concept of mini-batch processing, where the batch size, denoted as ‘n,’ falls between values greater than 1 and less than the total number of samples.\nHaving reviewed these key concepts, I believe we’ve laid the foundation for a better understanding. Now, let’s proceed to recommend the most effective approach.\nDiscovering the optimal combination of learning rate and batch size typically involves a trial-and-error process. It demands a deep comprehension of the specific dataset, model architecture, and the problem domain. To embark on this journey, initiating with either a grid search or random search can systematically explore various combinations.\nHere are some suggestions to guide this exploration:\n1.Large Batch Size with Small Learning Rate :- When employing a large batch size, it’s advisable to accompany it with a smaller learning rate to prevent overshooting.\nA smaller learning rate means that the model updates its parameters more cautiously after each batch of data. This helps the model to converge more smoothly and avoid overshooting the optimal parameter values.\nUsing a large batch size (i.e., processing multiple data points simultaneously) can lead to a more stable gradient estimation. This stability can help in achieving better generalization because the model is exposed to a more representative sample of the training data during each update.\n2.Small Batch Size with Large Learning Rate:\nA large learning rate causes the model’s parameters to update more substantially after each batch. This can lead to faster convergence because the model explores the parameter space more aggressively.\nWith a small batch size, you’re updating the model’s parameters more frequently, which can help speed up convergence even further. Each batch contributes to an update, so the model can start improving quickly.\n*Key takeaway\n\nKey takeaway\n\nSmall learning rate with large batch size ——–&gt;Good\nSmall learning rate with small batch size ——–&gt;Bad\nSmall Batch Size with large Learning Rate ——–&gt;Good\nSmall Batch Size with small Learning Rate ——–&gt;Bad\n\n\nThis is a post with executable code."
  },
  {
    "objectID": "posts/post-with-code/index.html#learning-rate-vs-batch-size",
    "href": "posts/post-with-code/index.html#learning-rate-vs-batch-size",
    "title": "Learning rate vs Batch size",
    "section": "",
    "text": "Having reviewed these key concepts, I believe we’ve laid the foundation for a better understanding. Now, let’s proceed to recommend the most effective approach.Discovering the optimal combination of learning rate and batch size typically involves a trial-and-error process. It demands a deep comprehension of the specific dataset, model architecture, and the problem domain. To embark on this journey, initiating with either a grid search or random search can systematically explore various combinations.Here are some suggestions to guide this exploration:\n1.Use Large Batch Size with Small Learning Rate :- A smaller learning rate means that the model updates its parameters more cautiously after each batch of data. This helps the model to converge more smoothly and avoid overshooting the optimal parameter values.\nUsing a large batch size (i.e., processing multiple data points simultaneously) can lead to a more stable gradient estimation. This stability can help in achieving better generalization because the model is exposed to a more representative sample of the training data during each update.\n2.Use Small Batch Size with Large Learning Rate: A large learning rate causes the model’s parameters to update more substantially after each batch. This can lead to faster convergence because the model explores the parameter space more aggressively.\nWith a small batch size, you’re updating the model’s parameters more frequently, which can help speed up convergence even further. Each batch contributes to an update, so the model can start improving quickly.\n\nKey takeaway\n\nSmall learning rate with large batch size ——–&gt; Good\nSmall learning rate with small batch size ——–&gt; Bad\nSmall Batch Size with large Learning Rate ——–&gt; Good\nSmall Batch Size with small Learning Rate ——–&gt; Bad"
  }
]